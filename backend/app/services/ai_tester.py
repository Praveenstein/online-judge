"""Service for generating AI tests and validating code.

This module uses an AI agent to generate test inputs and a reference solution
for a given problem description, then executes both user and reference code
to compare outputs.
"""

from __future__ import annotations

# Built-In Imports.
from typing import List, Dict, Any, Optional
import pydantic
import re

# External Imports.
from pydantic import BaseModel, Field
from pydantic_ai import Agent
from pydantic_ai.models.cerebras import CerebrasModel
from pydantic_ai.providers.cerebras import CerebrasProvider
from pydantic_ai.profiles.openai import OpenAIModelProfile
from pydantic_ai.exceptions import UnexpectedModelBehavior

# Local Imports.
from app.config import settings
from app.services.compiler import CodeExecutor
from app.schemas import AITestResult, AITestExecutionResponse

# ---------------------------------------------------------------------------
# Internal Schemas for AI Output
# ---------------------------------------------------------------------------

class GeneratedTestCase(BaseModel):
    """A single test case generated by the AI."""
    input_data: str = Field(..., description="The stdin string for the test case.")
    explanation: Optional[str] = Field(None, description="Optional explanation of what this test covers.")

class GeneratedProblemTests(BaseModel):
    """The structured output from the AI tester agent."""
    test_cases: List[GeneratedTestCase] = Field(..., description="A list of test cases.")
    function_name: str = Field(..., description="The name of the function to solve the problem (e.g. 'solve').")
    reference_solution: str = Field(..., description="A complete, correct Python solution (function definition + optional helper logic).")
    harness_code: str = Field(..., description="A Python script that reads stdin, parses inputs, calls the function, and prints the result.")

# ---------------------------------------------------------------------------
# Agent Configuration
# ---------------------------------------------------------------------------

SYSTEM_PROMPT: str = (
    "You are an expert competitive programmer and test engineer. "
    "Your goal is to generate high-quality test cases and a reference solution for a given DSA problem.\n\n"
    "Tasks:\n"
    "1. Analyze the problem description provided.\n"
    "2. Generate 5 diverse test cases (include edge cases like empty inputs, small/large values, etc.).\n"
    "3. Provide a correct, highly efficient Python reference solution that implements the function logic.\n"
    "4. Determine a standard function name (e.g., 'solve', 'twoSum').\n"
    "5. Provide a robust Python harness script. This harness MUST:\n"
    "   - Read all inputs from stdin.\n"
    "   - Parse the inputs into whichever format the function needs (integers, strings, lists, etc.).\n"
    "   - Handle multiple input styles if possible (e.g., space or newline separation).\n"
    "   - Call the function using the 'function_name' provided.\n"
    "   - If the function modifies its input in-place (like sorting), print the modified input after the call.\n"
    "   - Print the return value or the result directly. For lists, print them space-separated or in a standard format.\n\n"
    "Crucial Requirement: The harness script should assume the function definition is already present in the environment."
)

def _make_tester_agent() -> Agent[None, GeneratedProblemTests]:
    """Build the AI agent that generates test cases."""
    cerebras_key = getattr(settings, "CEREBRAS_API_KEY", "") or ""
    if not cerebras_key:
        raise ValueError("CEREBRAS_API_KEY is not set.")

    model_name = getattr(settings, "CEREBRAS_MODEL", "") or "gpt-oss-120b"
    model = CerebrasModel(
        model_name,
        provider=CerebrasProvider(api_key=cerebras_key),
    )
    # Disable strict tool definitions to avoid "mixed values for strict" error
    model._profile = OpenAIModelProfile(openai_supports_strict_tool_definition=False)

    return Agent(
        model,
        output_type=GeneratedProblemTests,
        system_prompt=SYSTEM_PROMPT,
        retries=3
    )

# ---------------------------------------------------------------------------
# Service Logic
# ---------------------------------------------------------------------------

async def run_ai_tests(
    problem_description: str,
    user_code: str,
    language: str
) -> AITestExecutionResponse:
    """Generate tests via AI and execute them against user and reference code."""
    
    # Try to extract all class and function names to handle helpers/boilerplate/LeetCode style
    found_classes = re.findall(r"class\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*[:\(]", user_code)
    found_functions = re.findall(r"def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\(", user_code)
    
    agent = _make_tester_agent()
    # Generate tests and reference solution, providing hint(s) for the context
    classes_str = ", ".join(f"'{c}'" for c in found_classes) if found_classes else "None found"
    functions_str = ", ".join(f"'{f}'" for f in found_functions) if found_functions else "None found"
    
    prompt = (
        f"Problem Description:\n{problem_description}\n\n"
        f"Classes detected in user code: [{classes_str}]\n"
        f"Functions detected in user code: [{functions_str}]\n"
        "Task: Generate a harness that best fits the user's code structure.\n"
        "- If a class like 'Solution' is detected, the harness should instantiate it (e.g., `s = Solution()`) and call the method.\n"
        "- If only functions are detected, call the appropriate function directly.\n"
        "- The 'function_name' in your output should be the main method/function you are calling."
    )
    try:
        result = await agent.run(prompt)
    except UnexpectedModelBehavior as e:
        # UnexpectedModelBehavior often contains details about what went wrong
        raise ValueError(f"AI Test Generation Failed (Validation): {str(e)}")
    except Exception as e:
        raise ValueError(f"AI Test Generation Failed: {type(e).__name__}: {str(e)}")

    ai_data: GeneratedProblemTests = result.output
    
    test_results: List[AITestResult] = []
    passed_count = 0
    
    for test in ai_data.test_cases:
        # 1. Run Reference Solution to get expected output
        # Combine reference code (function def) and harness
        ref_full_code = f"{ai_data.reference_solution}\n\n{ai_data.harness_code}"
        ref_exec = await CodeExecutor.run("python", ref_full_code, test.input_data)
        
        if ref_exec.get("exit_code") != 0:
            # If reference solution fails, we might have a bad AI generation
            # We'll record this as a failure for the test case with a note
            expected_output = "ERROR: AI Reference Solution Failed"
            actual_output = ""
            passed = False
            stderr = f"Reference Error: {ref_exec.get('stderr')}"
        else:
            expected_output = ref_exec.get("stdout", "").strip()
            
            # 2. Run User Code
            # Append the AI's harness to the user's function definition
            user_full_code = f"{user_code}\n\n{ai_data.harness_code}"
            user_exec = await CodeExecutor.run(language, user_full_code, test.input_data)
            actual_output = user_exec.get("stdout", "").strip()
            stderr = user_exec.get("stderr")
            
            # 3. Validate
            exit_code = user_exec.get("exit_code", 1)
            # Match if exit code is 0 and output is same (ignoring trailing whitespace)
            passed = (exit_code == 0) and (actual_output == expected_output)
            
        if passed:
            passed_count += 1
            
        test_results.append(AITestResult(
            input_data=test.input_data,
            expected_output=expected_output,
            actual_output=actual_output,
            passed=passed,
            stderr=stderr,
            exit_code=user_exec.get("exit_code", 1) if 'user_exec' in locals() else 1
        ))
        
    summary = f"{passed_count}/{len(ai_data.test_cases)} Tests Passed"
    
    return AITestExecutionResponse(
        results=test_results,
        summary=summary
    )
